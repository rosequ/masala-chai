{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dspy\n",
    "from dotenv import load_dotenv\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"darrow-ai/LegalLensNER\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = raw_datasets.map(\n",
    "    lambda x: {\n",
    "        \"tokens\": ast.literal_eval(x[\"tokens\"]),\n",
    "        \"ner_tags\": ast.literal_eval(x[\"ner_tags\"]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': Value(dtype='string', id=None),\n",
       " 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '009c87b5-8608-4518-9ec1-ac1a1709d831',\n",
       " 'tokens': ['man',\n",
       "  ',',\n",
       "  'i',\n",
       "  'cant',\n",
       "  'believe',\n",
       "  'what',\n",
       "  'happened',\n",
       "  'recently',\n",
       "  '.',\n",
       "  'some',\n",
       "  'company',\n",
       "  'got',\n",
       "  'busted',\n",
       "  'for',\n",
       "  'breaking',\n",
       "  'the',\n",
       "  'can-spam',\n",
       "  'act',\n",
       "  '.',\n",
       "  'they',\n",
       "  'were',\n",
       "  'sending',\n",
       "  'out',\n",
       "  'promotional',\n",
       "  'emails',\n",
       "  'without',\n",
       "  'getting',\n",
       "  'permission',\n",
       "  'first',\n",
       "  '.',\n",
       "  'it',\n",
       "  'was',\n",
       "  'the',\n",
       "  'company',\n",
       "  'who',\n",
       "  'thought',\n",
       "  'they',\n",
       "  'could',\n",
       "  'get',\n",
       "  'away',\n",
       "  'with',\n",
       "  'it',\n",
       "  ',',\n",
       "  'but',\n",
       "  'they',\n",
       "  'were',\n",
       "  'wrong',\n",
       "  '.',\n",
       "  'they',\n",
       "  'were',\n",
       "  'doing',\n",
       "  'this',\n",
       "  'to',\n",
       "  'regular',\n",
       "  'folks',\n",
       "  'like',\n",
       "  'you',\n",
       "  'and',\n",
       "  'me',\n",
       "  '.',\n",
       "  'not',\n",
       "  'cool',\n",
       "  '.'],\n",
       " 'ner_tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-LAW',\n",
       "  'I-LAW',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'I-VIOLATION',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-VIOLATED BY',\n",
       "  'I-VIOLATED BY',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'I-VIOLATED ON',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another', 'case', 'that', 'caught', 'my', 'attention', 'was', 'a', 'violation', 'of', 'employment', 'laws', 'by', 'a', 'popular', 'talent', 'agency', '.', 'they', 'were', 'found', 'guilty', 'of', 'requiring', 'their', 'non-union', 'employees', ',', 'including', 'the', 'plaintiff', ',', 'to', 'work', 'more', 'than', '31.33', 'hours', 'per', 'week', 'without', 'providing', 'overtime', 'compensation', '.', 'this', 'was', 'a', 'clear', 'violation', 'committed', 'on', 'their', 'non-union', 'employees', '.', 'its', 'disheartening', 'to', 'see', 'such', 'practices', 'in', 'the', 'entertainment', 'industry', '.']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LAW', 'I-LAW', 'O', 'O', 'B-VIOLATED BY', 'I-VIOLATED BY', 'I-VIOLATED BY', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'I-VIOLATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-VIOLATED ON', 'I-VIOLATED ON', 'I-VIOLATED ON', 'I-VIOLATED ON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0]['tokens'])\n",
    "print(raw_datasets['train'][0]['ner_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(dataset):\n",
    "    label_set = set()\n",
    "    for data in dataset:\n",
    "        labels = data[\"ner_tags\"]\n",
    "        label_set.update(labels)\n",
    "    return list(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, label_to_id):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, example_labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        last_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != last_word_id:\n",
    "                label_id = label_to_id.get(example_labels[word_id], -100)\n",
    "                label_ids.append(label_id)\n",
    "            else:\n",
    "                label_ids.append(\n",
    "                    -100\n",
    "                )\n",
    "            last_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = get_label_list(\n",
    "    raw_datasets[\"train\"]\n",
    ")  # Assuming 'train' split exists and contains the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space = True, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label_to_id mapping\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=len(label_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 710/710 [00:00<00:00, 3773.92 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 617/617 [00:00<00:00, 4074.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization and alignment of labels\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer, label_to_id), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"distilbert-finetuned-ner\",\n",
    "                         evaluation_strategy = \"epoch\",\n",
    "                         save_strategy=\"epoch\",\n",
    "                         learning_rate = 2e-5,\n",
    "                         num_train_epochs=10,\n",
    "                         warmup_steps=500,\n",
    "                         weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 10%|â–ˆ         | 89/890 [00:55<05:58,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.0579078197479248, 'eval_LAW_precision': 0.0, 'eval_LAW_recall': 0.0, 'eval_LAW_f1': 0.0, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.0, 'eval_VIOLATED BY_recall': 0.0, 'eval_VIOLATED BY_f1': 0.0, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.0, 'eval_VIOLATED ON_recall': 0.0, 'eval_VIOLATED ON_f1': 0.0, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.0, 'eval_VIOLATION_recall': 0.0, 'eval_VIOLATION_f1': 0.0, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.8041627469426152, 'eval_runtime': 11.9809, 'eval_samples_per_second': 51.499, 'eval_steps_per_second': 6.51, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 178/890 [01:37<05:02,  2.35it/s]/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 20%|â–ˆâ–ˆ        | 178/890 [01:49<05:02,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35483449697494507, 'eval_LAW_precision': 0.024390243902439025, 'eval_LAW_recall': 0.013333333333333334, 'eval_LAW_f1': 0.017241379310344827, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.0, 'eval_VIOLATED BY_recall': 0.0, 'eval_VIOLATED BY_f1': 0.0, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.0, 'eval_VIOLATED ON_recall': 0.0, 'eval_VIOLATED ON_f1': 0.0, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.1680129240710824, 'eval_VIOLATION_recall': 0.33766233766233766, 'eval_VIOLATION_f1': 0.2243797195253506, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.163408913213448, 'eval_overall_recall': 0.24851367419738407, 'eval_overall_f1': 0.1971698113207547, 'eval_overall_accuracy': 0.8875117591721543, 'eval_runtime': 11.639, 'eval_samples_per_second': 53.011, 'eval_steps_per_second': 6.702, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 267/890 [02:41<04:23,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2878110706806183, 'eval_LAW_precision': 0.5135135135135135, 'eval_LAW_recall': 0.5066666666666667, 'eval_LAW_f1': 0.5100671140939598, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.56, 'eval_VIOLATED BY_recall': 0.56, 'eval_VIOLATED BY_f1': 0.56, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.26666666666666666, 'eval_VIOLATED ON_recall': 0.26666666666666666, 'eval_VIOLATED ON_f1': 0.26666666666666666, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.30457380457380456, 'eval_VIOLATION_recall': 0.47564935064935066, 'eval_VIOLATION_f1': 0.37135614702154623, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.33136593591905567, 'eval_overall_recall': 0.46730083234244946, 'eval_overall_f1': 0.3877651702022693, 'eval_overall_accuracy': 0.9069849482596425, 'eval_runtime': 11.5662, 'eval_samples_per_second': 53.345, 'eval_steps_per_second': 6.744, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 356/890 [03:34<03:45,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2678896188735962, 'eval_LAW_precision': 0.7761194029850746, 'eval_LAW_recall': 0.6933333333333334, 'eval_LAW_f1': 0.7323943661971832, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.72, 'eval_VIOLATED BY_recall': 0.48, 'eval_VIOLATED BY_f1': 0.576, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.4626865671641791, 'eval_VIOLATED ON_recall': 0.41333333333333333, 'eval_VIOLATED ON_f1': 0.43661971830985913, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.36929460580912865, 'eval_VIOLATION_recall': 0.43344155844155846, 'eval_VIOLATION_f1': 0.39880507841672885, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.4255788313120176, 'eval_overall_recall': 0.4589774078478002, 'eval_overall_f1': 0.4416475972540046, 'eval_overall_accuracy': 0.9202963311382879, 'eval_runtime': 11.6148, 'eval_samples_per_second': 53.122, 'eval_steps_per_second': 6.716, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 445/890 [04:27<03:07,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2803806662559509, 'eval_LAW_precision': 0.8169014084507042, 'eval_LAW_recall': 0.7733333333333333, 'eval_LAW_f1': 0.7945205479452055, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.7076923076923077, 'eval_VIOLATED BY_recall': 0.6133333333333333, 'eval_VIOLATED BY_f1': 0.657142857142857, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.45588235294117646, 'eval_VIOLATED ON_recall': 0.41333333333333333, 'eval_VIOLATED ON_f1': 0.43356643356643354, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.41356382978723405, 'eval_VIOLATION_recall': 0.5048701298701299, 'eval_VIOLATION_f1': 0.4546783625730994, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.4665271966527197, 'eval_overall_recall': 0.5303210463733651, 'eval_overall_f1': 0.4963828603227602, 'eval_overall_accuracy': 0.925376293508937, 'eval_runtime': 11.6068, 'eval_samples_per_second': 53.158, 'eval_steps_per_second': 6.72, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 500/890 [04:52<02:57,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5895, 'grad_norm': 6.033874034881592, 'learning_rate': 2e-05, 'epoch': 5.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 534/890 [05:20<02:34,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3175104856491089, 'eval_LAW_precision': 0.7096774193548387, 'eval_LAW_recall': 0.88, 'eval_LAW_f1': 0.7857142857142856, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.43884892086330934, 'eval_VIOLATED BY_recall': 0.8133333333333334, 'eval_VIOLATED BY_f1': 0.5700934579439252, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.31896551724137934, 'eval_VIOLATED ON_recall': 0.49333333333333335, 'eval_VIOLATED ON_f1': 0.387434554973822, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.35900216919739697, 'eval_VIOLATION_recall': 0.5373376623376623, 'eval_VIOLATION_f1': 0.43042912873862166, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.38976377952755903, 'eval_overall_recall': 0.5885850178359097, 'eval_overall_f1': 0.46897205116058743, 'eval_overall_accuracy': 0.9174976481655691, 'eval_runtime': 11.6709, 'eval_samples_per_second': 52.866, 'eval_steps_per_second': 6.683, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 623/890 [06:14<01:53,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3269725441932678, 'eval_LAW_precision': 0.7439024390243902, 'eval_LAW_recall': 0.8133333333333334, 'eval_LAW_f1': 0.7770700636942677, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.45588235294117646, 'eval_VIOLATED BY_recall': 0.8266666666666667, 'eval_VIOLATED BY_f1': 0.5876777251184834, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.27672955974842767, 'eval_VIOLATED ON_recall': 0.5866666666666667, 'eval_VIOLATED ON_f1': 0.37606837606837606, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.343030303030303, 'eval_VIOLATION_recall': 0.4594155844155844, 'eval_VIOLATION_f1': 0.3927827897293546, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.37437603993344426, 'eval_overall_recall': 0.535077288941736, 'eval_overall_f1': 0.44052863436123346, 'eval_overall_accuracy': 0.9226716839134524, 'eval_runtime': 11.5954, 'eval_samples_per_second': 53.211, 'eval_steps_per_second': 6.727, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 712/890 [07:06<01:15,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35967200994491577, 'eval_LAW_precision': 0.7252747252747253, 'eval_LAW_recall': 0.88, 'eval_LAW_f1': 0.7951807228915663, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.5213675213675214, 'eval_VIOLATED BY_recall': 0.8133333333333334, 'eval_VIOLATED BY_f1': 0.6354166666666666, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.4423076923076923, 'eval_VIOLATED ON_recall': 0.6133333333333333, 'eval_VIOLATED ON_f1': 0.5139664804469273, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.3844221105527638, 'eval_VIOLATION_recall': 0.4967532467532468, 'eval_VIOLATION_f1': 0.43342776203966005, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.4323104693140794, 'eval_overall_recall': 0.5695600475624257, 'eval_overall_f1': 0.49153412006157005, 'eval_overall_accuracy': 0.9253292568203199, 'eval_runtime': 11.5797, 'eval_samples_per_second': 53.283, 'eval_steps_per_second': 6.736, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 801/890 [07:59<00:37,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3485467731952667, 'eval_LAW_precision': 0.67, 'eval_LAW_recall': 0.8933333333333333, 'eval_LAW_f1': 0.7657142857142857, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.5213675213675214, 'eval_VIOLATED BY_recall': 0.8133333333333334, 'eval_VIOLATED BY_f1': 0.6354166666666666, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.4380952380952381, 'eval_VIOLATED ON_recall': 0.6133333333333333, 'eval_VIOLATED ON_f1': 0.5111111111111111, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.38901869158878505, 'eval_VIOLATION_recall': 0.5405844155844156, 'eval_VIOLATION_f1': 0.452445652173913, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.4303904923599321, 'eval_overall_recall': 0.6028537455410226, 'eval_overall_f1': 0.5022288261515601, 'eval_overall_accuracy': 0.9255409219190969, 'eval_runtime': 11.6124, 'eval_samples_per_second': 53.133, 'eval_steps_per_second': 6.717, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890/890 [08:52<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3735874891281128, 'eval_LAW_precision': 0.6504854368932039, 'eval_LAW_recall': 0.8933333333333333, 'eval_LAW_f1': 0.752808988764045, 'eval_LAW_number': 75, 'eval_VIOLATED BY_precision': 0.49193548387096775, 'eval_VIOLATED BY_recall': 0.8133333333333334, 'eval_VIOLATED BY_f1': 0.6130653266331659, 'eval_VIOLATED BY_number': 75, 'eval_VIOLATED ON_precision': 0.4, 'eval_VIOLATED ON_recall': 0.6133333333333333, 'eval_VIOLATED ON_f1': 0.48421052631578954, 'eval_VIOLATED ON_number': 75, 'eval_VIOLATION_precision': 0.4062111801242236, 'eval_VIOLATION_recall': 0.5308441558441559, 'eval_VIOLATION_f1': 0.4602392681210415, 'eval_VIOLATION_number': 616, 'eval_overall_precision': 0.43679163034001744, 'eval_overall_recall': 0.5957193816884662, 'eval_overall_f1': 0.5040241448692154, 'eval_overall_accuracy': 0.9258701787394168, 'eval_runtime': 11.6145, 'eval_samples_per_second': 53.123, 'eval_steps_per_second': 6.716, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 890/890 [08:53<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 533.1982, 'train_samples_per_second': 13.316, 'train_steps_per_second': 1.669, 'train_loss': 0.34481533029106226, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=890, training_loss=0.34481533029106226, metrics={'train_runtime': 533.1982, 'train_samples_per_second': 13.316, 'train_steps_per_second': 1.669, 'total_flos': 927754419916800.0, 'train_loss': 0.34481533029106226, 'epoch': 10.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:13<00:00,  5.94it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_result = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3735874891281128,\n",
       " 'eval_LAW_precision': 0.6504854368932039,\n",
       " 'eval_LAW_recall': 0.8933333333333333,\n",
       " 'eval_LAW_f1': 0.752808988764045,\n",
       " 'eval_LAW_number': 75,\n",
       " 'eval_VIOLATED BY_precision': 0.49193548387096775,\n",
       " 'eval_VIOLATED BY_recall': 0.8133333333333334,\n",
       " 'eval_VIOLATED BY_f1': 0.6130653266331659,\n",
       " 'eval_VIOLATED BY_number': 75,\n",
       " 'eval_VIOLATED ON_precision': 0.4,\n",
       " 'eval_VIOLATED ON_recall': 0.6133333333333333,\n",
       " 'eval_VIOLATED ON_f1': 0.48421052631578954,\n",
       " 'eval_VIOLATED ON_number': 75,\n",
       " 'eval_VIOLATION_precision': 0.4062111801242236,\n",
       " 'eval_VIOLATION_recall': 0.5308441558441559,\n",
       " 'eval_VIOLATION_f1': 0.4602392681210415,\n",
       " 'eval_VIOLATION_number': 616,\n",
       " 'eval_overall_precision': 0.43679163034001744,\n",
       " 'eval_overall_recall': 0.5957193816884662,\n",
       " 'eval_overall_f1': 0.5040241448692154,\n",
       " 'eval_overall_accuracy': 0.9258701787394168,\n",
       " 'eval_runtime': 13.5628,\n",
       " 'eval_samples_per_second': 45.492,\n",
       " 'eval_steps_per_second': 5.751,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v for k, v in eval_result.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
