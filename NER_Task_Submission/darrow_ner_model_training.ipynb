{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log into HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /Users/khalidrajan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "hf_token = os.environ[\"HF_TOKEN\"]\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"darrow-ai/LegalLensNER-SharedTask\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_literal_eval(value):\n",
    "    try:\n",
    "        return ast.literal_eval(value)\n",
    "    except (ValueError, SyntaxError):\n",
    "        # Return None\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string representations of lists into actual lists\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\n",
    "        \"tokens\": safe_literal_eval(x[\"tokens\"]),\n",
    "        \"ner_tags\": safe_literal_eval(x[\"ner_tags\"]),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_none(example):\n",
    "    # Check that 'tokens' and 'ner_tags' fields are not None\n",
    "    return example[\"tokens\"] is not None and example[\"ner_tags\"] is not None\n",
    "\n",
    "# Filter the dataset to remove examples with None values in 'tokens' or 'ner_tags'\n",
    "dataset = dataset.filter(is_not_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_matching_len(example):\n",
    "    # Check that the len of tokens matches the len of NER tags\n",
    "    return len(example[\"tokens\"]) == len(example[\"ner_tags\"])\n",
    "\n",
    "dataset = dataset.filter(non_matching_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and eval sets\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.3, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 679\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 291\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_list(dataset: Dataset):\n",
    "    label_set = set()\n",
    "    for data in dataset:\n",
    "        labels = data[\"ner_tags\"]\n",
    "        label_set.update(labels)\n",
    "    return list(label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = get_label_list(\n",
    "    dataset[\"train\"]\n",
    ")  # Assuming 'train' split exists and contains the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-VIOLATION',\n",
       " 'I-VIOLATED ON',\n",
       " 'B-VIOLATED BY',\n",
       " 'I-VIOLATED BY',\n",
       " 'B-VIOLATED ON',\n",
       " 'B-LAW',\n",
       " 'I-LAW',\n",
       " 'I-VIOLATION']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    labels = []\n",
    "    for i, example_labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        last_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_id != last_word_id:\n",
    "                label_id = label_to_id.get(example_labels[word_id], -100)\n",
    "                label_ids.append(label_id)\n",
    "            else:\n",
    "                label_ids.append(label_id)\n",
    "            last_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"FacebookAI/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space = True, use_fast = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 679/679 [00:00<00:00, 3739.01 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 291/291 [00:00<00:00, 4009.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    lambda x: tokenize_and_align_labels(x, tokenizer), batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label = id_to_label, label2id = label_to_id)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "                        output_dir=f\"{model_checkpoint}_legal_ner_finetuned\",\n",
    "                        evaluation_strategy = \"epoch\",\n",
    "                        save_strategy=\"epoch\",\n",
    "                        learning_rate = 2e-5,\n",
    "                        num_train_epochs=10,\n",
    "                        warmup_steps=500,\n",
    "                        weight_decay=0.01)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 85/850 [01:17<11:09,  1.14it/s]/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                \n",
      " 10%|â–ˆ         | 85/850 [01:26<11:09,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7163046598434448, 'eval_LAW_precision': 0.0, 'eval_LAW_recall': 0.0, 'eval_LAW_f1': 0.0, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.0, 'eval_VIOLATED BY_recall': 0.0, 'eval_VIOLATED BY_f1': 0.0, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.0, 'eval_VIOLATED ON_recall': 0.0, 'eval_VIOLATED ON_f1': 0.0, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.0, 'eval_VIOLATION_recall': 0.0, 'eval_VIOLATION_f1': 0.0, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.0, 'eval_overall_recall': 0.0, 'eval_overall_f1': 0.0, 'eval_overall_accuracy': 0.7706214689265537, 'eval_runtime': 9.6732, 'eval_samples_per_second': 30.083, 'eval_steps_per_second': 3.825, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 170/850 [02:46<09:50,  1.15it/s] /Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 20%|â–ˆâ–ˆ        | 170/850 [02:56<09:50,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37249431014060974, 'eval_LAW_precision': 0.0, 'eval_LAW_recall': 0.0, 'eval_LAW_f1': 0.0, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.0, 'eval_VIOLATED BY_recall': 0.0, 'eval_VIOLATED BY_f1': 0.0, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.0, 'eval_VIOLATED ON_recall': 0.0, 'eval_VIOLATED ON_f1': 0.0, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.25389755011135856, 'eval_VIOLATION_recall': 0.3048128342245989, 'eval_VIOLATION_f1': 0.27703523693803156, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.25389755011135856, 'eval_overall_recall': 0.18506493506493507, 'eval_overall_f1': 0.21408450704225349, 'eval_overall_accuracy': 0.8888233849177106, 'eval_runtime': 9.7226, 'eval_samples_per_second': 29.93, 'eval_steps_per_second': 3.806, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 255/850 [04:14<08:24,  1.18it/s]/Users/khalidrajan/anaconda3/envs/llm_dev_python3.8/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "                                                 \n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 255/850 [04:23<08:24,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2795945107936859, 'eval_LAW_precision': 0.3541666666666667, 'eval_LAW_recall': 0.1588785046728972, 'eval_LAW_f1': 0.21935483870967742, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.0, 'eval_VIOLATED BY_recall': 0.0, 'eval_VIOLATED BY_f1': 0.0, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.0, 'eval_VIOLATED ON_recall': 0.0, 'eval_VIOLATED ON_f1': 0.0, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.4166666666666667, 'eval_VIOLATION_recall': 0.48128342245989303, 'eval_VIOLATION_f1': 0.4466501240694789, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.40451745379876797, 'eval_overall_recall': 0.3198051948051948, 'eval_overall_f1': 0.35720761559383496, 'eval_overall_accuracy': 0.9129943502824859, 'eval_runtime': 9.5472, 'eval_samples_per_second': 30.48, 'eval_steps_per_second': 3.875, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 340/850 [05:50<07:16,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20830805599689484, 'eval_LAW_precision': 0.7692307692307693, 'eval_LAW_recall': 0.7476635514018691, 'eval_LAW_f1': 0.7582938388625592, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.72, 'eval_VIOLATED BY_recall': 0.5070422535211268, 'eval_VIOLATED BY_f1': 0.5950413223140496, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.46938775510204084, 'eval_VIOLATED ON_recall': 0.359375, 'eval_VIOLATED ON_f1': 0.4070796460176991, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.5339805825242718, 'eval_VIOLATION_recall': 0.5882352941176471, 'eval_VIOLATION_f1': 0.5597964376590331, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.583739837398374, 'eval_overall_recall': 0.5827922077922078, 'eval_overall_f1': 0.5832656376929326, 'eval_overall_accuracy': 0.9391795627609923, 'eval_runtime': 9.4967, 'eval_samples_per_second': 30.642, 'eval_steps_per_second': 3.896, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 425/850 [07:16<05:50,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.18790946900844574, 'eval_LAW_precision': 0.8130841121495327, 'eval_LAW_recall': 0.8130841121495327, 'eval_LAW_f1': 0.8130841121495327, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.6984126984126984, 'eval_VIOLATED BY_recall': 0.6197183098591549, 'eval_VIOLATED BY_f1': 0.6567164179104478, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.5254237288135594, 'eval_VIOLATED ON_recall': 0.484375, 'eval_VIOLATED ON_f1': 0.5040650406504066, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.5714285714285714, 'eval_VIOLATION_recall': 0.6524064171122995, 'eval_VIOLATION_f1': 0.6092384519350811, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.6189024390243902, 'eval_overall_recall': 0.6590909090909091, 'eval_overall_f1': 0.6383647798742139, 'eval_overall_accuracy': 0.9440923605993613, 'eval_runtime': 9.0141, 'eval_samples_per_second': 32.283, 'eval_steps_per_second': 4.105, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 500/850 [08:21<04:58,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5159, 'grad_norm': 10.536821365356445, 'learning_rate': 2e-05, 'epoch': 5.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 510/850 [08:39<04:40,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2666742205619812, 'eval_LAW_precision': 0.7627118644067796, 'eval_LAW_recall': 0.8411214953271028, 'eval_LAW_f1': 0.7999999999999999, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.6901408450704225, 'eval_VIOLATED BY_recall': 0.6901408450704225, 'eval_VIOLATED BY_f1': 0.6901408450704225, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.5, 'eval_VIOLATED ON_recall': 0.375, 'eval_VIOLATED ON_f1': 0.42857142857142855, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.5943877551020408, 'eval_VIOLATION_recall': 0.6229946524064172, 'eval_VIOLATION_f1': 0.608355091383812, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.629570747217806, 'eval_overall_recall': 0.6428571428571429, 'eval_overall_f1': 0.6361445783132531, 'eval_overall_accuracy': 0.9382461311717023, 'eval_runtime': 9.0112, 'eval_samples_per_second': 32.293, 'eval_steps_per_second': 4.106, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 595/850 [10:02<03:30,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2121937870979309, 'eval_LAW_precision': 0.8198198198198198, 'eval_LAW_recall': 0.8504672897196262, 'eval_LAW_f1': 0.8348623853211008, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.7619047619047619, 'eval_VIOLATED BY_recall': 0.676056338028169, 'eval_VIOLATED BY_f1': 0.7164179104477612, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.5230769230769231, 'eval_VIOLATED ON_recall': 0.53125, 'eval_VIOLATED ON_f1': 0.5271317829457365, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.6171284634760705, 'eval_VIOLATION_recall': 0.6550802139037433, 'eval_VIOLATION_f1': 0.635538261997406, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.6572327044025157, 'eval_overall_recall': 0.6785714285714286, 'eval_overall_f1': 0.6677316293929713, 'eval_overall_accuracy': 0.949987718005404, 'eval_runtime': 8.9948, 'eval_samples_per_second': 32.352, 'eval_steps_per_second': 4.113, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 680/850 [11:25<02:18,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.244528129696846, 'eval_LAW_precision': 0.8571428571428571, 'eval_LAW_recall': 0.8411214953271028, 'eval_LAW_f1': 0.8490566037735849, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.7142857142857143, 'eval_VIOLATED BY_recall': 0.7746478873239436, 'eval_VIOLATED BY_f1': 0.7432432432432432, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.5074626865671642, 'eval_VIOLATED ON_recall': 0.53125, 'eval_VIOLATED ON_f1': 0.5190839694656488, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.6047619047619047, 'eval_VIOLATION_recall': 0.679144385026738, 'eval_VIOLATION_f1': 0.6397984886649873, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.6472346786248132, 'eval_overall_recall': 0.702922077922078, 'eval_overall_f1': 0.6739299610894942, 'eval_overall_accuracy': 0.9493981822647998, 'eval_runtime': 8.9903, 'eval_samples_per_second': 32.368, 'eval_steps_per_second': 4.116, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 765/850 [12:48<01:09,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2678690552711487, 'eval_LAW_precision': 0.8468468468468469, 'eval_LAW_recall': 0.8785046728971962, 'eval_LAW_f1': 0.8623853211009174, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.85, 'eval_VIOLATED BY_recall': 0.7183098591549296, 'eval_VIOLATED BY_f1': 0.7786259541984734, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.4647887323943662, 'eval_VIOLATED ON_recall': 0.515625, 'eval_VIOLATED ON_f1': 0.4888888888888889, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.6170212765957447, 'eval_VIOLATION_recall': 0.6978609625668449, 'eval_VIOLATION_f1': 0.6549560853199498, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.6601503759398496, 'eval_overall_recall': 0.7126623376623377, 'eval_overall_f1': 0.6854020296643247, 'eval_overall_accuracy': 0.9500859739621714, 'eval_runtime': 9.0043, 'eval_samples_per_second': 32.318, 'eval_steps_per_second': 4.109, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 850/850 [14:13<00:00,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25197145342826843, 'eval_LAW_precision': 0.8773584905660378, 'eval_LAW_recall': 0.8691588785046729, 'eval_LAW_f1': 0.8732394366197183, 'eval_LAW_number': 107, 'eval_VIOLATED BY_precision': 0.8225806451612904, 'eval_VIOLATED BY_recall': 0.7183098591549296, 'eval_VIOLATED BY_f1': 0.7669172932330828, 'eval_VIOLATED BY_number': 71, 'eval_VIOLATED ON_precision': 0.53125, 'eval_VIOLATED ON_recall': 0.53125, 'eval_VIOLATED ON_f1': 0.53125, 'eval_VIOLATED ON_number': 64, 'eval_VIOLATION_precision': 0.6624685138539043, 'eval_VIOLATION_recall': 0.7032085561497327, 'eval_VIOLATION_f1': 0.682230869001297, 'eval_VIOLATION_number': 374, 'eval_overall_precision': 0.7011128775834659, 'eval_overall_recall': 0.7159090909090909, 'eval_overall_f1': 0.708433734939759, 'eval_overall_accuracy': 0.9545566199950872, 'eval_runtime': 9.0247, 'eval_samples_per_second': 32.245, 'eval_steps_per_second': 4.1, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 850/850 [14:14<00:00,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 854.6281, 'train_samples_per_second': 7.945, 'train_steps_per_second': 0.995, 'train_loss': 0.3191476922876695, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=850, training_loss=0.3191476922876695, metrics={'train_runtime': 854.6281, 'train_samples_per_second': 7.945, 'train_steps_per_second': 0.995, 'total_flos': 1774317261588480.0, 'train_loss': 0.3191476922876695, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [00:08<00:00,  4.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# evaluate model on eval set\n",
    "evaluate_results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.25197145342826843,\n",
       " 'eval_LAW_precision': 0.8773584905660378,\n",
       " 'eval_LAW_recall': 0.8691588785046729,\n",
       " 'eval_LAW_f1': 0.8732394366197183,\n",
       " 'eval_LAW_number': 107,\n",
       " 'eval_VIOLATED BY_precision': 0.8225806451612904,\n",
       " 'eval_VIOLATED BY_recall': 0.7183098591549296,\n",
       " 'eval_VIOLATED BY_f1': 0.7669172932330828,\n",
       " 'eval_VIOLATED BY_number': 71,\n",
       " 'eval_VIOLATED ON_precision': 0.53125,\n",
       " 'eval_VIOLATED ON_recall': 0.53125,\n",
       " 'eval_VIOLATED ON_f1': 0.53125,\n",
       " 'eval_VIOLATED ON_number': 64,\n",
       " 'eval_VIOLATION_precision': 0.6624685138539043,\n",
       " 'eval_VIOLATION_recall': 0.7032085561497327,\n",
       " 'eval_VIOLATION_f1': 0.682230869001297,\n",
       " 'eval_VIOLATION_number': 374,\n",
       " 'eval_overall_precision': 0.7011128775834659,\n",
       " 'eval_overall_recall': 0.7159090909090909,\n",
       " 'eval_overall_f1': 0.708433734939759,\n",
       " 'eval_overall_accuracy': 0.9545566199950872,\n",
       " 'eval_runtime': 9.1573,\n",
       " 'eval_samples_per_second': 31.778,\n",
       " 'eval_steps_per_second': 4.04,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push our fine tuned model to the HuggingFace hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "training_args.bin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.24k/5.24k [00:00<00:00, 45.1kB/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 496M/496M [00:28<00:00, 17.7MB/s]\n",
      "Upload 2 LFS files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/khalidrajan/roberta-base_legal_ner_finetuned/commit/f5ec09e1ca25e0b86392573437c5dba40218ba8f', commit_message='End of training', commit_description='', oid='f5ec09e1ca25e0b86392573437c5dba40218ba8f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev_python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
