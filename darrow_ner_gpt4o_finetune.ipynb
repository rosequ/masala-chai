{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Path to your JSONL file\n",
    "file_path = 'train_data.json'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_list[:700], data_list[701:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file in write mode\n",
    "with open('train_data.jsonl', 'w') as outfile:\n",
    "    for data in train_data:\n",
    "        # Combine tokens and ner_tags into space-separated strings\n",
    "        prompt = ' '.join(data['tokens'])\n",
    "        completion = ' '.join(data['ner_tags'])\n",
    "        \n",
    "        # Create the JSONL entry\n",
    "        jsonl_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are performing Named Entity Recognition (NER) to extract legal entities such as violation, violation by, violation on, and law from the text.  You will return a list of IOB tags.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": completion}\n",
    "                ]}\n",
    "        \n",
    "        # Write the JSONL entry to the file\n",
    "        json.dump(jsonl_entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file in write mode\n",
    "with open('eval_data.jsonl', 'w') as outfile:\n",
    "    for data in test_data:\n",
    "        # Combine tokens and ner_tags into space-separated strings\n",
    "        prompt = ' '.join(data['tokens'])\n",
    "        completion = ' '.join(data['ner_tags'])\n",
    "        \n",
    "        # Create the JSONL entry\n",
    "        jsonl_entry = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are performing Named Entity Recognition (NER) to extract legal entities such as violation, violation by, violation on, and law from the text.  You will return a list of IOB tags.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": completion}\n",
    "                ]}\n",
    "        # Write the JSONL entry to the file\n",
    "        json.dump(jsonl_entry, outfile)\n",
    "        outfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-WIru8EaROpmFFgbahYdJEsiP', bytes=656999, created_at=1724716608, filename='train_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(\n",
    "  file=open(\"train_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileObject(id='file-9ZOez4FskUrENFCChyWzScsV', bytes=258376, created_at=1724716617, filename='eval_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.files.create(\n",
    "  file=open(\"eval_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-MReXReh8762EVcIQXlckSB6z', created_at=1724716652, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-Qxm4Gb8DM4gPh1hlxL8eCrsO', result_files=[], seed=1908658738, status='validating_files', trained_tokens=None, training_file='file-WIru8EaROpmFFgbahYdJEsiP', validation_file='file-9ZOez4FskUrENFCChyWzScsV', estimated_finish=None, integrations=[], user_provided_suffix=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.fine_tuning.jobs.create(\n",
    "  training_file=\"file-WIru8EaROpmFFgbahYdJEsiP\", \n",
    "  model=\"gpt-4o-2024-08-06\",\n",
    "  validation_file=\"file-9ZOez4FskUrENFCChyWzScsV\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FineTuningJob(id='ftjob-MReXReh8762EVcIQXlckSB6z', created_at=1724716652, error=Error(code=None, message=None, param=None), fine_tuned_model='ft:gpt-4o-2024-08-06:georgian::A0eqm7lq', finished_at=1724720391, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=2), model='gpt-4o-2024-08-06', object='fine_tuning.job', organization_id='org-Qxm4Gb8DM4gPh1hlxL8eCrsO', result_files=['file-9EcuXYpvXxkRqnMVhib50rU5'], seed=1908658738, status='succeeded', trained_tokens=509676, training_file='file-WIru8EaROpmFFgbahYdJEsiP', validation_file='file-9ZOez4FskUrENFCChyWzScsV', estimated_finish=None, integrations=[], user_provided_suffix=None)\n"
     ]
    }
   ],
   "source": [
    "fine_tune_job = client.fine_tuning.jobs.retrieve(\"ftjob-MReXReh8762EVcIQXlckSB6z\")\n",
    "print(fine_tune_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14243437</td>\n",
       "      <td>[\"a\",\"class\",\"action\",\"lawsuit\",\"has\",\"been\",\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18871551</td>\n",
       "      <td>[\"a\",\"media\",\"company\",\"recently\",\"came\",\"unde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51514749</td>\n",
       "      <td>[\"a\",\"national\",\"bank\",\"was\",\"recently\",\"held\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99676183</td>\n",
       "      <td>[\"a\",\"recent\",\"case\",\"has\",\"come\",\"to\",\"light\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14188948</td>\n",
       "      <td>[\"a\",\"recent\",\"incident\",\"has\",\"come\",\"to\",\"li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>24828975</td>\n",
       "      <td>[\"the\",\"defendants\",\"have\",\"been\",\"accused\",\"o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>69861119</td>\n",
       "      <td>[\"the\",\"plaintiff\",\"has\",\"been\",\"subjected\",\"t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>97047066</td>\n",
       "      <td>[\"the\",\"plaintiff\",\"has\",\"raised\",\"serious\",\"a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>90651371</td>\n",
       "      <td>[\"the\",\"plaintiffs,\",\"representing\",\"a\",\"group...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>3066104</td>\n",
       "      <td>[\"the\",\"tenant\",\"had\",\"a\",\"regrettable\",\"exper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>380 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                             tokens\n",
       "0    14243437  [\"a\",\"class\",\"action\",\"lawsuit\",\"has\",\"been\",\"...\n",
       "1    18871551  [\"a\",\"media\",\"company\",\"recently\",\"came\",\"unde...\n",
       "2    51514749  [\"a\",\"national\",\"bank\",\"was\",\"recently\",\"held\"...\n",
       "3    99676183  [\"a\",\"recent\",\"case\",\"has\",\"come\",\"to\",\"light\"...\n",
       "4    14188948  [\"a\",\"recent\",\"incident\",\"has\",\"come\",\"to\",\"li...\n",
       "..        ...                                                ...\n",
       "375  24828975  [\"the\",\"defendants\",\"have\",\"been\",\"accused\",\"o...\n",
       "376  69861119  [\"the\",\"plaintiff\",\"has\",\"been\",\"subjected\",\"t...\n",
       "377  97047066  [\"the\",\"plaintiff\",\"has\",\"raised\",\"serious\",\"a...\n",
       "378  90651371  [\"the\",\"plaintiffs,\",\"representing\",\"a\",\"group...\n",
       "379   3066104  [\"the\",\"tenant\",\"had\",\"a\",\"regrettable\",\"exper...\n",
       "\n",
       "[380 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"NER_test_set.xlsx\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(tokens: list[str]) -> list[str]:\n",
    "    input_str = ' '.join(tokens)\n",
    "    \n",
    "    output = client.chat.completions.create(\n",
    "        model=\"ft:gpt-4o-2024-08-06:georgian::A0eqm7lq\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are performing Named Entity Recognition (NER) to detect legal violations on a provided list of tokens.  You will return a list of IOB tags.\"},\n",
    "            {\"role\": \"user\", \"content\": input_str}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    output_str = output.choices[0].message.content\n",
    "    \n",
    "    ner_tags = output_str.split()\n",
    "    \n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"ner_tags\"] = df[\"tokens\"].apply(model_inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"NER_test_set_results_gpt4o_finetuned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/royalsequiera/Projects/Darrow-LegalLens-NER/NER_Task_Submission/predictions_NERLens.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>predicted_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14243437</td>\n",
       "      <td>[\"a\",\"class\",\"action\",\"lawsuit\",\"has\",\"been\",\"...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18871551</td>\n",
       "      <td>[\"a\",\"media\",\"company\",\"recently\",\"came\",\"unde...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51514749</td>\n",
       "      <td>[\"a\",\"national\",\"bank\",\"was\",\"recently\",\"held\"...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99676183</td>\n",
       "      <td>[\"a\",\"recent\",\"case\",\"has\",\"come\",\"to\",\"light\"...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14188948</td>\n",
       "      <td>[\"a\",\"recent\",\"incident\",\"has\",\"come\",\"to\",\"li...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                             tokens  \\\n",
       "0  14243437  [\"a\",\"class\",\"action\",\"lawsuit\",\"has\",\"been\",\"...   \n",
       "1  18871551  [\"a\",\"media\",\"company\",\"recently\",\"came\",\"unde...   \n",
       "2  51514749  [\"a\",\"national\",\"bank\",\"was\",\"recently\",\"held\"...   \n",
       "3  99676183  [\"a\",\"recent\",\"case\",\"has\",\"come\",\"to\",\"light\"...   \n",
       "4  14188948  [\"a\",\"recent\",\"incident\",\"has\",\"come\",\"to\",\"li...   \n",
       "\n",
       "                                      predicted_tags  \n",
       "0  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "1  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "3  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "4  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def format_token_tags_from_df(df):\n",
    "    formatted_data = []\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Safely evaluate strings that look like lists\n",
    "        tokens = row['tokens']\n",
    "        predicted_tags = row['predicted_tags']\n",
    "        \n",
    "        if isinstance(tokens, str):\n",
    "            tokens = ast.literal_eval(tokens)\n",
    "        if isinstance(predicted_tags, str):\n",
    "            predicted_tags = ast.literal_eval(predicted_tags)\n",
    "        \n",
    "        token_tag_pairs = [f\"{token}/{tag}\" for token, tag in zip(tokens, predicted_tags)]\n",
    "        formatted_string = ', '.join(token_tag_pairs)\n",
    "        formatted_data.append({'id': row['id'], 'formatted': formatted_string})\n",
    "\n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 40410727,\n",
       "  'formatted': 'the/O, lead/O, plaintiff/O, alleges/O, that/O, the/O, app/O, has/O, been/O, issuing/B-VIOLATION, false/I-VIOLATION, and/I-VIOLATION, misleading/I-VIOLATION, financial/I-VIOLATION, statements,/I-VIOLATION, thereby/I-VIOLATION, artificially/I-VIOLATION, inflating/I-VIOLATION, the/I-VIOLATION, price/I-VIOLATION, of/I-VIOLATION, its/I-VIOLATION, securities/I-VIOLATION, ./O, this/O, unethical/O, behavior/O, was/O, orchestrated/O, by/O, the/O, apps/O, lead/O, developer/O, and/O, has/O, raised/O, serious/O, doubts/O, about/O, the/O, apps/O, reliability/O, ./O'},\n",
       " {'id': 61979338,\n",
       "  'formatted': 'a/O, restaurant/O, was/O, found/O, guilty/O, of/O, violating/O, the/O, fair/B-LAW, labor/I-LAW, standards/I-LAW, act/I-LAW, ./O, the/O, court/O, found/O, that/O, the/B-VIOLATED BY, restaurant/I-VIOLATED BY, management/I-VIOLATED BY, had/O, failed/O, to/O, comply/O, with/O, the/O, strict/O, notification/I-LAW, requirements/I-LAW, for/O, claiming/O, a/O, tip/O, credit,/O, including/O, notifying/O, employees/I-VIOLATION, of/O, the/O, cash/I-VIOLATION, wage,/I-VIOLATION, tip/I-VIOLATION, credit/I-VIOLATION, amount,/I-VIOLATION, limitations/I-VIOLATION, on/I-VIOLATION, tip/I-VIOLATION, credit,/I-VIOLATION, retention/I-VIOLATION, of/I-VIOLATION, tips,/I-VIOLATION, and/O, the/O, application/I-VIOLATION, of/I-VIOLATION, tip/I-VIOLATION, credit/I-VIOLATION, ./O, this/O, violation/O, was/O, committed/O, on/B-VIOLATED ON, the/I-VIOLATED ON, restaurants/I-VIOLATED ON, tipped/I-VIOLATED ON, employees/I-VIOLATED ON, ./O'}]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_token_tags_from_df(df.sample(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER File Check: NER prediction file format is correct.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_ner_format(predictions_file_path, test_file_path):\n",
    "    \"\"\"\n",
    "    Check the format of the NER prediction file.\n",
    "    The file should be in CSV format with columns: id, tokens, ner_tags\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(predictions_file_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading predictions CSV file: {e}\"\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading test CSV file: {e}\"\n",
    "    \n",
    "    # Check expected columns\n",
    "    expected_columns = ['id', 'tokens', 'ner_tags']\n",
    "    pred_columns = list(df.columns)\n",
    "    for expected_col in expected_columns:\n",
    "        if expected_col not in pred_columns:\n",
    "            return False, f\"Incorrect columns. Expected: {expected_columns}, Found: {pred_columns}\"\n",
    "    \n",
    "    # Check number of rows\n",
    "    expected_ner_num_rows = len(test_df)\n",
    "    predictions_ner_num_rows = len(df)\n",
    "    if predictions_ner_num_rows != expected_ner_num_rows:\n",
    "        return False, f\"Incorrect number of predictions. Expected: {expected_ner_num_rows}, Found: {predictions_ner_num_rows}\"\n",
    "\n",
    "    return True, \"NER prediction file format is correct.\"\n",
    "\n",
    "def check_nli_format(predictions_file_path, test_file_path):\n",
    "    \"\"\"\n",
    "    Check the format of the NLI prediction file.\n",
    "    The file should be in CSV format with columns: Premise, hypothesis, label\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(predictions_file_path)\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading predictions CSV file: {e}\"\n",
    "    \n",
    "    try:\n",
    "        test_df = pd.read_csv(test_file_path)\n",
    "    except Exception as e:\n",
    "        return False, f\"Error reading test CSV file: {e}\"\n",
    "    \n",
    "    # Check expected columns\n",
    "    expected_columns = ['premise', 'hypothesis', 'label']\n",
    "    pred_columns = list(df.columns)\n",
    "    for expected_col in expected_columns:\n",
    "        if expected_col not in pred_columns:\n",
    "            return False, f\"Incorrect columns. Expected: {expected_columns}, Found: {pred_columns}\"\n",
    "    \n",
    "    # Check number of rows\n",
    "    expected_nli_num_rows = len(test_df)\n",
    "    predictions_nli_num_rows = len(df)\n",
    "    if predictions_nli_num_rows != expected_nli_num_rows:\n",
    "        return False, f\"Incorrect number of predictions. Expected: {expected_nli_num_rows}, Found: {predictions_nli_num_rows}\"\n",
    "    \n",
    "    return True, \"NLI prediction file format is correct.\"\n",
    "\n",
    "# Check NER prediction file\n",
    "ner_predictions_file_path = '/Users/royalsequiera/Projects/Darrow-LegalLens-NER/NER_Task_Submission/predictions_NERLens.csv' # replace with file path\n",
    "ner_test_file_path = '/Users/royalsequiera/Projects/Darrow-LegalLens-NER/NER_Task_Submission/test_NERLens.csv' # replace with file path\n",
    "is_valid, message = check_ner_format(ner_predictions_file_path, ner_test_file_path)\n",
    "print(f\"NER File Check: {message}\")\n",
    "\n",
    "# # Check NLI prediction file\n",
    "# nli_predictions_file_path = 'predictions_NLILens.csv' # replace with file path\n",
    "# nli_test_file_path = 'predictions_NLILens.csv' # replace with file path\n",
    "# is_valid, message = check_nli_format(nli_predictions_file_path, nli_test_file_path)\n",
    "# print(f\"NLI File Check: {message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "excel_file_path = '/Users/royalsequiera/Projects/Darrow-LegalLens-NER/NER_Task_Submission/NER_test_set.xlsx'\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "df = pd.read_excel(excel_file_path)\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'output_file.csv'\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
